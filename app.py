import streamlit as st
from pypdf import PdfReader
import google.generativeai as genai
import io
import pandas as pd
from datetime import datetime
import hashlib

# Konfiguration mit Fallback
def get_credentials():
    try:
        return {"jl_team": st.secrets["JL_PASSWORD"]}
    except:
        return {"jl_team": "junge_liberale_2025"}  # Fallback fÃ¼r Testing

TEAM_CREDENTIALS = get_credentials()

# Database-Funktionen
def save_analysis_to_db(pdf_name: str, analysis_text: str, full_text: str):
    """Analyseergebnis in CSV-Database speichern"""
    try:
        # Eindeutige ID fÃ¼r Artikel basierend auf Text-Hash
        article_hash = hashlib.md5(full_text.encode()).hexdigest()[:12]
        
        # Daten fÃ¼r CSV
        data = {
            'id': article_hash,
            'datum': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'pdf_name': pdf_name,
            'analyse': analysis_text,
            'volltext_kurz': full_text[:500] + "..." if len(full_text) > 500 else full_text
        }
        
        # CSV laden oder erstellen
        try:
            df = pd.read_csv('jl_artikel_database.csv')
        except:
            df = pd.DataFrame(columns=['id', 'datum', 'pdf_name', 'analyse', 'volltext_kurz'])
        
        # Duplikat-Check
        if article_hash not in df['id'].values:
            df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)
            df.to_csv('jl_artikel_database.csv', index=False)
            return True
        return False
        
    except Exception as e:
        st.error(f"Database-Fehler: {e}")
        return False

def load_article_database():
    """Artikel-Database laden"""
    try:
        return pd.read_csv('jl_artikel_database.csv')
    except:
        return pd.DataFrame(columns=['id', 'datum', 'pdf_name', 'analyse', 'volltext_kurz'])

def search_articles(query: str, df: pd.DataFrame):
    """Artikel durchsuchen"""
    if query:
        mask = df['analyse'].str.contains(query, case=False, na=False) | \
               df['pdf_name'].str.contains(query, case=False, na=False) | \
               df['volltext_kurz'].str.contains(query, case=False, na=False)
        return df[mask]
    return df

def extract_pdf_text(pdf_file) -> str:
    """PDF-Text extrahieren mit verbesserter Multi-Page UnterstÃ¼tzung"""
    try:
        pdf_reader = PdfReader(io.BytesIO(pdf_file.read()))
        
        # Debug-Info anzeigen
        total_pages = len(pdf_reader.pages)
        st.info(f"ğŸ“„ PDF hat {total_pages} Seiten")
        
        text = ""
        page_texts = []
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            page_texts.append(f"=== SEITE {page_num} ===\n{page_text}\n")
            text += page_text + "\n\n"
            
            # Debug: Zeige Text-LÃ¤nge pro Seite
            st.write(f"Seite {page_num}: {len(page_text)} Zeichen")
        
        # Gesamt-Info
        st.success(f"âœ… Extrahiert: {len(text)} Zeichen aus {total_pages} Seiten")
        
        # Zeige ersten Teil zur Kontrolle
        with st.expander("ğŸ” Extrahierter Text (erste 1000 Zeichen)"):
            st.text(text[:1000] + "..." if len(text) > 1000 else text)
        
        return text
        
    except Exception as e:
        st.error(f"PDF-Fehler: {e}")
        return ""

def analyze_with_gemini(text: str, api_key: str) -> str:
    """Text mit Google Gemini analysieren - mit Chunking fÃ¼r lange Texte"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Text-LÃ¤nge prÃ¼fen
        text_length = len(text)
        st.info(f"ğŸ“ Text-LÃ¤nge: {text_length} Zeichen")
        
        # Gemini 1.5 Flash kann ~1M Tokens = ~4M Zeichen
        # Aber fÃ¼r Sicherheit chunken wir bei 50k Zeichen
        max_chunk_size = 50000
        
        if text_length <= max_chunk_size:
            # Kurzer Text - normale Analyse
            st.info("âœ… Text passt in ein StÃ¼ck - normale Analyse")
            return analyze_single_chunk(text, model)
        else:
            # Langer Text - in Chunks aufteilen
            st.warning(f"âš ï¸ Text zu lang ({text_length} Zeichen) - wird in Teile aufgeteilt")
            return analyze_chunked_text(text, model, max_chunk_size)
        
    except Exception as e:
        return f"âŒ **Analyse-Fehler:** {str(e)}"

def analyze_single_chunk(text: str, model) -> str:
    """Einzelnen Text-Chunk analysieren"""
    prompt = f"""
    AUFTRAG: Analysiere diesen Zeitungstext und kategorisiere alle gefundenen Artikel fÃ¼r die Jungen Liberalen.

    KATEGORIEN:
    ğŸ”¥ HÃ–CHSTE PRIORITÃ„T (Sofort handeln):
    - Kommunalpolitik (Stadtrat, BÃ¼rgermeister, lokale Wahlen)
    - Wirtschaft & Gewerbe (Ansiedlungen, ArbeitsplÃ¤tze, Startups)
    - Bildung (Schulen, Unis, Digitalisierung)
    - Verkehr & Infrastruktur (Ã–PNV, Radwege, StraÃŸen)

    âš¡ HOHE PRIORITÃ„T (Wichtig fÃ¼r JL):
    - Digitalisierung & Innovation
    - Umwelt & Nachhaltigkeit (pragmatische LÃ¶sungen)
    - BÃ¼rgerbeteiligung & Demokratie
    - Jugendthemen

    ğŸ“° STANDARD (Beobachten):
    - Kultur & Events
    - Sport
    - Soziales
    - Sonstiges

    FORMAT:
    FÃ¼r jeden Artikel:
    **[KATEGORIE]** - Ãœberschrift
    ğŸ“ Kurze Zusammenfassung (1-2 SÃ¤tze)
    ğŸ¯ JL-Relevanz: Warum wichtig fÃ¼r Junge Liberale
    ---

    TEXT:
    {text}
    """
    
    response = model.generate_content(prompt)
    return response.text

def analyze_chunked_text(text: str, model, chunk_size: int) -> str:
    """Langen Text in Chunks aufteilen und analysieren"""
    
    # Text in Chunks aufteilen (versuche bei AbsÃ¤tzen zu trennen)
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        end_pos = min(current_pos + chunk_size, len(text))
        
        # Versuche bei Absatz oder Satz zu trennen
        if end_pos < len(text):
            # Suche letzten Absatz in diesem Chunk
            last_paragraph = text.rfind('\n\n', current_pos, end_pos)
            if last_paragraph > current_pos:
                end_pos = last_paragraph
            else:
                # Falls kein Absatz, suche letzten Satz
                last_sentence = text.rfind('.', current_pos, end_pos)
                if last_sentence > current_pos:
                    end_pos = last_sentence + 1
        
        chunk = text[current_pos:end_pos].strip()
        if chunk:
            chunks.append(chunk)
        
        current_pos = end_pos
    
    st.info(f"ğŸ“„ Text aufgeteilt in {len(chunks)} Teile")
    
    # Jeden Chunk einzeln analysieren
    all_analyses = []
    
    for i, chunk in enumerate(chunks, 1):
        st.write(f"ğŸ” Analysiere Teil {i}/{len(chunks)}...")
        
        chunk_prompt = f"""
        AUFTRAG: Analysiere diesen Zeitungstext-Teil und kategorisiere alle gefundenen Artikel fÃ¼r die Jungen Liberalen.
        WICHTIG: Dies ist Teil {i} von {len(chunks)} - analysiere nur die vollstÃ¤ndigen Artikel in diesem Teil.

        KATEGORIEN:
        ğŸ”¥ HÃ–CHSTE PRIORITÃ„T: Kommunalpolitik, Wirtschaft & Gewerbe, Bildung, Verkehr & Infrastruktur
        âš¡ HOHE PRIORITÃ„T: Digitalisierung & Innovation, Umwelt & Nachhaltigkeit, BÃ¼rgerbeteiligung & Demokratie, Jugendthemen
        ğŸ“° STANDARD: Kultur & Events, Sport, Soziales, Sonstiges

        FORMAT:
        **[KATEGORIE]** - Ãœberschrift
        ğŸ“ Zusammenfassung
        ğŸ¯ JL-Relevanz
        ---

        TEXT TEIL {i}:
        {chunk}
        """
        
        try:
            response = model.generate_content(chunk_prompt)
            chunk_analysis = response.text
            all_analyses.append(f"## ğŸ“„ TEIL {i}/{len(chunks)}\n\n{chunk_analysis}")
        except Exception as e:
            all_analyses.append(f"## ğŸ“„ TEIL {i}/{len(chunks)}\n\nâŒ Fehler bei Teil {i}: {e}")
    
    # Alle Analysen zusammenfassen
    final_analysis = f"""
# ğŸ“° VOLLSTÃ„NDIGE ZEITUNGSANALYSE
*Analysiert in {len(chunks)} Teilen wegen TextlÃ¤nge*

{chr(10).join(all_analyses)}

---
**ğŸ“Š ZUSAMMENFASSUNG:** {len(chunks)} Teile analysiert, {len(text)} Zeichen Gesamttext
"""
    
    return final_analysis

def show_login():
    """Login-Seite anzeigen"""
    st.title("ğŸ” JL Zeitungsanalyse - Login")
    
    with st.form("login_form"):
        username = st.text_input("ğŸ‘¤ Benutzername:")
        password = st.text_input("ğŸ”’ Passwort:", type="password")
        submit = st.form_submit_button("ğŸš€ Einloggen")
        
        if submit:
            if username in TEAM_CREDENTIALS and TEAM_CREDENTIALS[username] == password:
                st.session_state.logged_in = True
                st.session_state.username = username
                st.rerun()
            else:
                st.error("âŒ Falsche Anmeldedaten!")
    
    st.info("ğŸ’¡ **Demo-Zugang:** jl_team / junge_liberale_2025")

def analyze_tab():
    """Tab fÃ¼r neue Artikel-Analyse"""
    st.header("ğŸ“¤ PDF-Upload & Analyse")
    
    # API Setup mit besserer Behandlung
    api_key = None
    
    # Versuche API-Key aus Secrets
    try:
        api_key = st.secrets.get("GEMINI_API_KEY", "")
        if api_key:
            st.sidebar.success("âœ… API-Key aus Konfiguration geladen")
    except:
        pass
    
    # Falls nicht in Secrets, dann Sidebar-Eingabe
    if not api_key:
        with st.sidebar:
            st.header("âš™ï¸ API-Konfiguration")
            api_key = st.text_input(
                "Google Gemini API-Key:", 
                type="password",
                help="Kostenlos bei https://makersuite.google.com/app/apikey"
            )
            
            if api_key:
                st.success("âœ… API-Key gesetzt")
            else:
                st.warning("âš ï¸ API-Key benÃ¶tigt")
                st.info("ğŸ“– https://makersuite.google.com/app/apikey")
    
    # PDF Upload
    pdf_file = st.file_uploader(
        "ğŸ“„ Zeitungs-PDF hochladen:",
        type=['pdf'],
        help="Lokale Zeitungen, Gemeindeblatts, etc."
    )
    
    if pdf_file:
        st.success(f"âœ… **{pdf_file.name}** hochgeladen ({pdf_file.size} Bytes)")
        
        # PDF analysieren
        if st.button("ğŸ” Zeitung analysieren", type="primary"):
            if api_key:
                with st.spinner("ğŸ“– PDF wird gelesen..."):
                    text = extract_pdf_text(pdf_file)
                
                if text.strip():
                    with st.spinner("ğŸ¤– KI analysiert Artikel..."):
                        analysis = analyze_with_gemini(text, api_key)
                    
                    # Ergebnis anzeigen
                    st.success("âœ… Analyse abgeschlossen!")
                    st.markdown("---")
                    st.markdown("## ğŸ“‹ Analyseergebnis:")
                    st.markdown(analysis)
                    
                    # In Database speichern
                    if save_analysis_to_db(pdf_file.name, analysis, text):
                        st.success("ğŸ’¾ Artikel in Database gespeichert!")
                    else:
                        st.info("â„¹ï¸ Artikel bereits in Database vorhanden")
                    
                    # Download-Option
                    st.download_button(
                        label="ğŸ“¥ Analyse herunterladen",
                        data=f"# JL Zeitungsanalyse - {pdf_file.name}\n\n{analysis}",
                        file_name=f"JL_Analyse_{pdf_file.name}_{datetime.now().strftime('%Y%m%d')}.md",
                        mime="text/markdown"
                    )
                else:
                    st.error("âŒ Kein Text im PDF gefunden!")
            else:
                st.warning("âš ï¸ API-Key benÃ¶tigt!")

def search_tab():
    """Tab fÃ¼r Artikel-Suche in der Database"""
    st.header("ğŸ” Artikel-Database durchsuchen")
    
    # Database laden
    df = load_article_database()
    
    if df.empty:
        st.info("ğŸ“­ Noch keine Artikel in der Database. Analysiere zuerst ein paar PDFs!")
        return
    
    # Suchfunktionen
    col1, col2 = st.columns([3, 1])
    
    with col1:
        search_query = st.text_input(
            "ğŸ” Suche nach Themen, StichwÃ¶rtern oder PDF-Namen:",
            placeholder="z.B. Stadtrat, Digitalisierung, Verkehr..."
        )
    
    with col2:
        st.metric("ğŸ“Š Gesamt-Artikel", len(df))
    
    # Zeitfilter
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("ğŸ“… Von:", value=None)
    with col2:
        date_to = st.date_input("ğŸ“… Bis:", value=None)
    
    # Suche ausfÃ¼hren
    filtered_df = df.copy()
    
    if search_query:
        filtered_df = search_articles(search_query, filtered_df)
    
    if date_from:
        filtered_df = filtered_df[pd.to_datetime(filtered_df['datum']).dt.date >= date_from]
    if date_to:
        filtered_df = filtered_df[pd.to_datetime(filtered_df['datum']).dt.date <= date_to]
    
    # Ergebnisse anzeigen
    st.markdown(f"### ğŸ“‹ Gefunden: {len(filtered_df)} Artikel")
    
    if not filtered_df.empty:
        # Sortierung
        sort_by = st.selectbox("Sortieren nach:", ["Datum (neu-alt)", "PDF-Name", "Datum (alt-neu)"])
        
        if sort_by == "Datum (neu-alt)":
            filtered_df = filtered_df.sort_values('datum', ascending=False)
        elif sort_by == "PDF-Name":
            filtered_df = filtered_df.sort_values('pdf_name')
        else:
            filtered_df = filtered_df.sort_values('datum', ascending=True)
        
        # Artikel anzeigen - OHNE NESTED EXPANDERS
        for idx, row in filtered_df.iterrows():
            st.markdown("---")
            st.markdown(f"### ğŸ“° {row['pdf_name']}")
            st.markdown(f"**ğŸ“… Datum:** {row['datum']}")
            st.markdown("**ğŸ” Analyse:**")
            st.markdown(row['analyse'])
            
            # Volltext als Button + Text Area (NICHT als Expander)
            if st.button(f"ğŸ“– Volltext anzeigen/verstecken", key=f"toggle_{idx}"):
                if f"show_text_{idx}" not in st.session_state:
                    st.session_state[f"show_text_{idx}"] = True
                else:
                    st.session_state[f"show_text_{idx}"] = not st.session_state[f"show_text_{idx}"]
            
            if st.session_state.get(f"show_text_{idx}", False):
                st.markdown("**ğŸ“„ Volltext-Vorschau:**")
                st.text_area("", value=row['volltext_kurz'], height=150, key=f"text_{idx}", disabled=True)
        
        # Export-Option
        csv_data = filtered_df.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ Suchergebnisse als CSV exportieren",
            data=csv_data,
            file_name=f"JL_Artikel_Export_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )
    else:
        st.info("ğŸ” Keine Artikel gefunden. Versuche andere Suchbegriffe.")

def stats_tab():
    """Tab fÃ¼r Statistiken"""
    st.header("ğŸ“Š Artikel-Statistiken")
    
    df = load_article_database()
    
    if df.empty:
        st.info("ğŸ“­ Noch keine Daten fÃ¼r Statistiken verfÃ¼gbar.")
        return
    
    # Basis-Statistiken
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“° Gesamt-Artikel", len(df))
    
    with col2:
        if not df.empty:
            latest_date = pd.to_datetime(df['datum']).max().strftime('%d.%m.%Y')
            st.metric("ğŸ“… Letzte Analyse", latest_date)
    
    with col3:
        unique_pdfs = df['pdf_name'].nunique()
        st.metric("ğŸ“„ Verschiedene PDFs", unique_pdfs)
    
    with col4:
        avg_per_day = len(df) / max(1, (pd.to_datetime(df['datum']).max() - pd.to_datetime(df['datum']).min()).days + 1) if len(df) > 1 else len(df)
        st.metric("ğŸ“ˆ Ã˜ Artikel/Tag", f"{avg_per_day:.1f}")
    
    # Top PDFs
    st.markdown("### ğŸ† Meistanalysierte Zeitungen")
    pdf_counts = df['pdf_name'].value_counts().head(10)
    if not pdf_counts.empty:
        st.bar_chart(pdf_counts)
    
    # Keyword-Analyse (einfach)
    st.markdown("### ğŸ”¤ HÃ¤ufige Begriffe in Analysen")
    all_text = ' '.join(df['analyse'].astype(str))
    keywords = ['Stadtrat', 'BÃ¼rgermeister', 'Verkehr', 'Digitalisierung', 'Bildung', 'Wirtschaft', 'Umwelt', 'Kultur']
    keyword_counts = {kw: all_text.lower().count(kw.lower()) for kw in keywords}
    keyword_df = pd.DataFrame(list(keyword_counts.items()), columns=['Keyword', 'HÃ¤ufigkeit'])
    keyword_df = keyword_df[keyword_df['HÃ¤ufigkeit'] > 0].sort_values('HÃ¤ufigkeit', ascending=False)
    
    if not keyword_df.empty:
        st.bar_chart(keyword_df.set_index('Keyword'))

def main_app():
    """Hauptanwendung nach Login"""
    st.title("ğŸ“° JL Zeitungsanalyse")
    st.markdown("*Automatische Kategorisierung fÃ¼r Kommunalpolitik*")
    
    # Logout Button
    if st.button("ğŸšª Logout"):
        st.session_state.logged_in = False
        st.rerun()
    
    # Tab-Navigation
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ Neue Analyse", "ğŸ” Artikel-Suche", "ğŸ“Š Statistiken"])
    
    with tab1:
        analyze_tab()
    
    with tab2:
        search_tab()
        
    with tab3:
        stats_tab()

def main():
    """Hauptfunktion mit Session State Management"""
    
    # Session State initialisieren
    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
    
    # App-Routing
    if st.session_state.logged_in:
        main_app()
    else:
        show_login()

# App starten
if __name__ == "__main__":
    main()
