import streamlit as st
from pypdf import PdfReader
import google.generativeai as genai
import io
import pandas as pd
from datetime import datetime, timedelta
import hashlib
import re
import requests
import time
import base64

# Konfiguration mit Fallback
def get_credentials():
    try:
        return {"jl_team": st.secrets["JL_PASSWORD"]}
    except:
        return {"jl_team": "junge_liberale_2025"}  # Fallback f√ºr Testing

TEAM_CREDENTIALS = get_credentials()

# Database-Funktionen
def save_analysis_to_db(pdf_name: str, analysis_text: str, full_text: str):
    """Analyseergebnis in CSV-Database speichern"""
    try:
        # Eindeutige ID f√ºr Artikel basierend auf Text-Hash
        article_hash = hashlib.md5(full_text.encode()).hexdigest()[:12]
        
        # Daten f√ºr CSV
        data = {
            'id': article_hash,
            'datum': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'pdf_name': pdf_name,
            'analyse': analysis_text,
            'volltext_kurz': full_text[:500] + "..." if len(full_text) > 500 else full_text
        }
        
        # CSV laden oder erstellen
        try:
            df = pd.read_csv('jl_artikel_database.csv')
        except:
            df = pd.DataFrame(columns=['id', 'datum', 'pdf_name', 'analyse', 'volltext_kurz'])
        
        # Duplikat-Check
        if article_hash not in df['id'].values:
            df = pd.concat([df, pd.DataFrame([data])], ignore_index=True)
            df.to_csv('jl_artikel_database.csv', index=False)
            return True
        return False
        
    except Exception as e:
        st.error(f"Database-Fehler: {e}")
        return False

def load_article_database():
    """Artikel-Database laden"""
    try:
        return pd.read_csv('jl_artikel_database.csv')
    except:
        return pd.DataFrame(columns=['id', 'datum', 'pdf_name', 'analyse', 'volltext_kurz'])

def search_articles(query: str, df: pd.DataFrame):
    """Artikel durchsuchen"""
    if query:
        mask = df['analyse'].str.contains(query, case=False, na=False) | \
               df['pdf_name'].str.contains(query, case=False, na=False) | \
               df['volltext_kurz'].str.contains(query, case=False, na=False)
        return df[mask]
    return df

def extract_pdf_text(pdf_file) -> str:
    """PDF-Text extrahieren mit verbesserter Multi-Page Unterst√ºtzung"""
    try:
        pdf_reader = PdfReader(io.BytesIO(pdf_file.read()))
        
        # Debug-Info anzeigen
        total_pages = len(pdf_reader.pages)
        st.info(f"üìÑ PDF hat {total_pages} Seiten")
        
        text = ""
        page_texts = []
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            # F√ºge Seitenmarkierung hinzu
            page_texts.append(f"\n[SEITE {page_num}]\n{page_text}")
            text += f"\n[SEITE {page_num}]\n{page_text}\n"
        
        # Gesamt-Info
        st.success(f"‚úÖ Extrahiert: {len(text)} Zeichen aus {total_pages} Seiten")
        
        return text
        
    except Exception as e:
        st.error(f"PDF-Fehler: {e}")
        return ""

def analyze_with_gemini(text: str, api_key: str) -> str:
    """Text mit Google Gemini analysieren - mit Chunking f√ºr lange Texte"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Text-L√§nge pr√ºfen
        text_length = len(text)
        st.info(f"üìù Text-L√§nge: {text_length} Zeichen")
        
        # Gemini 1.5 Flash kann ~1M Tokens = ~4M Zeichen
        # Aber f√ºr Sicherheit chunken wir bei 50k Zeichen
        max_chunk_size = 50000
        
        if text_length <= max_chunk_size:
            # Kurzer Text - normale Analyse
            st.info("‚úÖ Text passt in ein St√ºck - normale Analyse")
            return analyze_complete_text(text, model)
        else:
            # Langer Text - in Chunks aufteilen
            st.warning(f"‚ö†Ô∏è Text zu lang ({text_length} Zeichen) - wird in Teile aufgeteilt")
            return analyze_chunked_text(text, model, max_chunk_size)
        
    except Exception as e:
        return f"‚ùå **Analyse-Fehler:** {str(e)}"

def analyze_complete_text(text: str, model) -> str:
    """Gesamten Text analysieren und formatiert ausgeben"""
    prompt = f"""
    AUFTRAG: Analysiere diesen Zeitungstext und finde NUR LOKALE/REGIONALE Artikel f√ºr die Jungen Liberalen.
    
    WICHTIG: 
    - NUR Artikel mit Bezug zu DESSAU-RO√üLAU oder SACHSEN-ANHALT
    - IGNORIERE Bundespolitik, internationale Themen, andere Bundesl√§nder
    - Zeige NUR Artikel mit H√ñCHSTER oder HOHER Priorit√§t
    - Extrahiere IMMER die Seitenzahl aus [SEITE X] Markierungen
    
    H√ñCHSTE PRIORIT√ÑT (üî•) - NUR LOKAL/REGIONAL:
    - Dessau-Ro√ülauer Stadtrat & Kommunalpolitik
    - Lokale Wirtschaft & Gewerbeansiedlungen in Dessau-Ro√ülau
    - Schulen & Bildung in Dessau-Ro√ülau und Sachsen-Anhalt
    - Lokaler Verkehr & Infrastruktur (Stra√üen, √ñPNV in Dessau)
    - Landespolitik Sachsen-Anhalt
    
    HOHE PRIORIT√ÑT (‚ö°) - NUR LOKAL/REGIONAL:
    - Digitalisierung in Dessau-Ro√ülau
    - Lokale Umwelt- & Nachhaltigkeitsprojekte
    - B√ºrgerbeteiligung in Dessau-Ro√ülau
    - Jugendthemen in der Region
    
    IGNORIERE KOMPLETT:
    - Bundespolitik (Bundestag, Bundesregierung, etc.)
    - Internationale Themen
    - Andere St√§dte/Bundesl√§nder (au√üer Sachsen-Anhalt)
    - Sport, Kultur (au√üer mit politischer Relevanz)
    
    FORMAT F√úR JEDEN ARTIKEL:
    ### [EMOJI] √úberschrift des Artikels
    **Seite:** [Nummer]
    **Kernaussage:** [1-2 S√§tze - Was ist die wichtigste Information?]
    **JuLi-Relevanz:** [1 Satz - Warum sollten JuLis hier aktiv werden?]
    
    ---
    
    GEBE NUR LOKALE/REGIONALE ARTIKEL AUS!
    
    TEXT:
    {text}
    """
    
    response = model.generate_content(prompt)
    return format_final_output(response.text)

def analyze_chunked_text(text: str, model, chunk_size: int) -> str:
    """Langen Text in Chunks aufteilen und analysieren"""
    
    # Text in Chunks aufteilen
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        end_pos = min(current_pos + chunk_size, len(text))
        
        # Versuche bei Seitenmarkierung zu trennen
        if end_pos < len(text):
            # Suche letzte Seitenmarkierung
            last_page_marker = text.rfind('[SEITE', current_pos, end_pos)
            if last_page_marker > current_pos:
                end_pos = last_page_marker
        
        chunk = text[current_pos:end_pos].strip()
        if chunk:
            chunks.append(chunk)
        
        current_pos = end_pos
    
    st.info(f"üìÑ Text aufgeteilt in {len(chunks)} Teile")
    
    # Sammle alle Artikel aus allen Chunks
    all_articles = []
    
    for i, chunk in enumerate(chunks, 1):
        with st.spinner(f"üîç Analysiere Teil {i}/{len(chunks)}..."):
            
            chunk_prompt = f"""
            AUFTRAG: Extrahiere NUR LOKALE/REGIONALE Artikel aus diesem Zeitungstext-Teil f√ºr die Jungen Liberalen.
            
            WICHTIG: 
            - NUR Artikel √ºber DESSAU-RO√üLAU oder SACHSEN-ANHALT
            - KEINE Bundespolitik oder internationale Themen
            - Nur H√ñCHSTE und HOHE Priorit√§t
            - Seitenzahlen aus [SEITE X] extrahieren
            
            H√ñCHSTE PRIORIT√ÑT: Dessau-Ro√ülauer Stadtrat, lokale Wirtschaft, Schulen in Dessau, Verkehr in Dessau, Landespolitik Sachsen-Anhalt
            HOHE PRIORIT√ÑT: Digitalisierung in Dessau, lokale Umweltprojekte, B√ºrgerbeteiligung Dessau, regionale Jugendthemen
            
            IGNORIERE: Bundespolitik, andere St√§dte/L√§nder, Sport, Kultur
            
            FORMAT PRO ARTIKEL:
            TITEL: [√úberschrift]
            SEITE: [Nummer]
            KATEGORIE: [H√∂chste/Hohe Priorit√§t]
            INHALT: [Kernaussage in 1-2 S√§tzen]
            RELEVANZ: [JuLi-Relevanz in 1 Satz]
            ===
            
            TEXT TEIL {i}:
            {chunk}
            """
            
            try:
                response = model.generate_content(chunk_prompt)
                # Parse die Artikel aus der Antwort
                articles = parse_articles_from_response(response.text)
                all_articles.extend(articles)
            except Exception as e:
                st.error(f"Fehler bei Teil {i}: {e}")
    
    # Erstelle finale Ausgabe
    return create_final_summary(all_articles)

def parse_articles_from_response(response_text: str) -> list:
    """Extrahiere strukturierte Artikel aus der KI-Antwort"""
    articles = []
    
    # Teile nach === auf
    raw_articles = response_text.split('===')
    
    for raw_article in raw_articles:
        if not raw_article.strip():
            continue
            
        article = {}
        lines = raw_article.strip().split('\n')
        
        for line in lines:
            if line.startswith('TITEL:'):
                article['titel'] = line.replace('TITEL:', '').strip()
            elif line.startswith('SEITE:'):
                article['seite'] = line.replace('SEITE:', '').strip()
            elif line.startswith('KATEGORIE:'):
                article['kategorie'] = line.replace('KATEGORIE:', '').strip()
            elif line.startswith('INHALT:'):
                article['inhalt'] = line.replace('INHALT:', '').strip()
            elif line.startswith('RELEVANZ:'):
                article['relevanz'] = line.replace('RELEVANZ:', '').strip()
        
        if 'titel' in article:  # Nur hinzuf√ºgen wenn Titel vorhanden
            articles.append(article)
    
    return articles

def create_final_summary(articles: list) -> str:
    """Erstelle finale formatierte Zusammenfassung"""
    if not articles:
        return "‚ùå Keine relevanten lokalen/regionalen Artikel gefunden."
    
    # Sortiere nach Priorit√§t
    hoechste = [a for a in articles if 'H√∂chste' in a.get('kategorie', '')]
    hohe = [a for a in articles if 'Hohe' in a.get('kategorie', '')]
    
    output = "# üì∞ ANALYSE-ERGEBNIS - DESSAU-RO√üLAU & SACHSEN-ANHALT\n\n"
    
    # Zusammenfassung
    output += f"**Gefunden:** {len(articles)} relevante lokale/regionale Artikel\n"
    output += f"- üî• H√∂chste Priorit√§t: {len(hoechste)}\n"
    output += f"- ‚ö° Hohe Priorit√§t: {len(hohe)}\n\n"
    output += "---\n\n"
    
    # H√∂chste Priorit√§t
    if hoechste:
        output += "## üî• H√ñCHSTE PRIORIT√ÑT - Sofort handeln!\n\n"
        for article in hoechste:
            output += format_article(article, "üî•")
    
    # Hohe Priorit√§t
    if hohe:
        output += "## ‚ö° HOHE PRIORIT√ÑT - Wichtig f√ºr JuLis\n\n"
        for article in hohe:
            output += format_article(article, "‚ö°")
    
    return output

def format_article(article: dict, emoji: str) -> str:
    """Formatiere einzelnen Artikel"""
    output = f"### {emoji} {article.get('titel', 'Unbekannter Titel')}\n"
    output += f"**üìÑ Seite:** {article.get('seite', 'k.A.')}\n"
    output += f"**üìç Kernaussage:** {article.get('inhalt', 'Keine Zusammenfassung verf√ºgbar')}\n"
    output += f"**üéØ JuLi-Relevanz:** {article.get('relevanz', 'Relevant f√ºr liberale Kommunalpolitik')}\n"
    output += "\n---\n\n"
    return output

def format_final_output(raw_output: str) -> str:
    """Formatiere die finale Ausgabe sch√∂ner"""
    # Z√§hle Artikel
    hoechste_count = raw_output.count("üî•")
    hohe_count = raw_output.count("‚ö°")
    
    # F√ºge Zusammenfassung hinzu
    summary = f"""# üì∞ ANALYSE-ERGEBNIS - DESSAU-RO√üLAU & SACHSEN-ANHALT

**Gefunden:** {hoechste_count + hohe_count} relevante lokale/regionale Artikel
- üî• H√∂chste Priorit√§t: {hoechste_count}
- ‚ö° Hohe Priorit√§t: {hohe_count}

üéØ **Fokus:** Nur Dessau-Ro√ülau und Sachsen-Anhalt
‚ùå **Ignoriert:** Bundespolitik & internationale Themen

---

"""
    
    return summary + raw_output

def show_login():
    """Login-Seite anzeigen"""
    st.title("üîê JL Zeitungsanalyse - Login")
    
    with st.form("login_form"):
        username = st.text_input("üë§ Benutzername:")
        password = st.text_input("üîí Passwort:", type="password")
        submit = st.form_submit_button("üöÄ Einloggen")
        
        if submit:
            if username in TEAM_CREDENTIALS and TEAM_CREDENTIALS[username] == password:
                st.session_state.logged_in = True
                st.session_state.username = username
                st.rerun()
            else:
                st.error("‚ùå Falsche Anmeldedaten!")
    
    st.info("üí° **Demo-Zugang:** jl_team / junge_liberale_2025")

def analyze_tab():
    """Tab f√ºr neue Artikel-Analyse"""
    st.header("üì§ PDF-Upload & Analyse")
    
    # API Setup mit besserer Behandlung
    api_key = None
    
    # Versuche API-Key aus Secrets
    try:
        api_key = st.secrets.get("GEMINI_API_KEY", "")
        if api_key:
            st.sidebar.success("‚úÖ API-Key aus Konfiguration geladen")
    except:
        pass
    
    # Falls nicht in Secrets, dann Sidebar-Eingabe
    if not api_key:
        with st.sidebar:
            st.header("‚öôÔ∏è API-Konfiguration")
            api_key = st.text_input(
                "Google Gemini API-Key:", 
                type="password",
                help="Kostenlos bei https://makersuite.google.com/app/apikey"
            )
            
            if api_key:
                st.success("‚úÖ API-Key gesetzt")
            else:
                st.warning("‚ö†Ô∏è API-Key ben√∂tigt")
                st.info("üìñ https://makersuite.google.com/app/apikey")
    
    # PDF Upload
    pdf_file = st.file_uploader(
        "üìÑ Zeitungs-PDF hochladen:",
        type=['pdf'],
        help="Lokale Zeitungen, Gemeindebl√§tter, etc."
    )
    
    if pdf_file:
        st.success(f"‚úÖ **{pdf_file.name}** hochgeladen ({pdf_file.size:,} Bytes)")
        
        # PDF analysieren
        if st.button("üîç Zeitung analysieren", type="primary", disabled=not api_key):
            if api_key:
                with st.spinner("üìñ PDF wird gelesen..."):
                    text = extract_pdf_text(pdf_file)
                
                if text.strip():
                    with st.spinner("ü§ñ KI analysiert relevante Artikel..."):
                        analysis = analyze_with_gemini(text, api_key)
                    
                    # Ergebnis anzeigen
                    st.success("‚úÖ Analyse abgeschlossen!")
                    st.markdown("---")
                    
                    # Analyse in sch√∂nem Format
                    st.markdown(analysis)
                    
                    # In Database speichern
                    if save_analysis_to_db(pdf_file.name, analysis, text):
                        st.success("üíæ Artikel in Database gespeichert!")
                    
                    # Download-Option
                    st.download_button(
                        label="üì• Analyse als Markdown herunterladen",
                        data=f"# JL Zeitungsanalyse - {pdf_file.name}\n\nDatum: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n{analysis}",
                        file_name=f"JL_Analyse_{pdf_file.name.replace('.pdf', '')}_{datetime.now().strftime('%Y%m%d')}.md",
                        mime="text/markdown"
                    )
                else:
                    st.error("‚ùå Kein Text im PDF gefunden!")
            else:
                st.warning("‚ö†Ô∏è Bitte API-Key in der Sidebar eingeben!")

def search_tab():
    """Tab f√ºr Artikel-Suche in der Database"""
    st.header("üîç Artikel-Database durchsuchen")
    
    # Database laden
    df = load_article_database()
    
    if df.empty:
        st.info("üì≠ Noch keine Artikel in der Database. Analysiere zuerst ein paar PDFs!")
        return
    
    # Suchfunktionen
    col1, col2 = st.columns([3, 1])
    
    with col1:
        search_query = st.text_input(
            "üîé Suche nach Themen, Stichw√∂rtern oder PDF-Namen:",
            placeholder="z.B. Stadtrat, Digitalisierung, Verkehr..."
        )
    
    with col2:
        st.metric("üìä Gesamt-Artikel", len(df))
    
    # Zeitfilter
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("üìÖ Von:", value=None)
    with col2:
        date_to = st.date_input("üìÖ Bis:", value=None)
    
    # Suche ausf√ºhren
    filtered_df = df.copy()
    
    if search_query:
        filtered_df = search_articles(search_query, filtered_df)
    
    if date_from:
        filtered_df = filtered_df[pd.to_datetime(filtered_df['datum']).dt.date >= date_from]
    if date_to:
        filtered_df = filtered_df[pd.to_datetime(filtered_df['datum']).dt.date <= date_to]
    
    # Ergebnisse anzeigen
    st.markdown(f"### üìã Gefunden: {len(filtered_df)} Artikel")
    
    if not filtered_df.empty:
        # Sortierung
        sort_by = st.selectbox("Sortieren nach:", ["Datum (neu‚Üíalt)", "PDF-Name", "Datum (alt‚Üíneu)"])
        
        if sort_by == "Datum (neu‚Üíalt)":
            filtered_df = filtered_df.sort_values('datum', ascending=False)
        elif sort_by == "PDF-Name":
            filtered_df = filtered_df.sort_values('pdf_name')
        else:
            filtered_df = filtered_df.sort_values('datum', ascending=True)
        
        # Artikel anzeigen
        for idx, row in filtered_df.iterrows():
            with st.container():
                st.markdown("---")
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.markdown(f"### üì∞ {row['pdf_name']}")
                with col2:
                    st.markdown(f"üìÖ {pd.to_datetime(row['datum']).strftime('%d.%m.%Y')}")
                
                # Analyse in Container
                with st.container():
                    st.markdown(row['analyse'])
        
        # Export-Option
        st.markdown("---")
        csv_data = filtered_df.to_csv(index=False)
        st.download_button(
            label="üì• Suchergebnisse als CSV exportieren",
            data=csv_data,
            file_name=f"JL_Artikel_Export_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )

def stats_tab():
    """Tab f√ºr Statistiken"""
    st.header("üìä Artikel-Statistiken")
    
    df = load_article_database()
    
    if df.empty:
        st.info("üì≠ Noch keine Daten f√ºr Statistiken verf√ºgbar.")
        return
    
    # Basis-Statistiken
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üì∞ Gesamt-Artikel", len(df))
    
    with col2:
        if not df.empty:
            latest_date = pd.to_datetime(df['datum']).max().strftime('%d.%m.%Y')
            st.metric("üìÖ Letzte Analyse", latest_date)
    
    with col3:
        unique_pdfs = df['pdf_name'].nunique()
        st.metric("üìÑ Verschiedene PDFs", unique_pdfs)
    
    with col4:
        # Artikel mit hoher/h√∂chster Priorit√§t z√§hlen
        high_prio_count = df['analyse'].str.contains('üî•|‚ö°', na=False).sum()
        st.metric("üéØ Relevante Artikel", high_prio_count)
    
    # Top Themen
    st.markdown("### üèÜ Top Themen in den Analysen")
    
    # Themen-Keywords definieren
    themes = {
        'Kommunalpolitik': ['Stadtrat', 'B√ºrgermeister', 'Gemeinderat', 'Kommune'],
        'Verkehr': ['Verkehr', '√ñPNV', 'Mobilit√§t', 'Stra√üe', 'Radweg'],
        'Digitalisierung': ['Digital', 'Internet', 'Online', 'IT'],
        'Bildung': ['Schule', 'Bildung', 'Universit√§t', 'Studium'],
        'Wirtschaft': ['Wirtschaft', 'Unternehmen', 'Gewerbe', 'Arbeitspl√§tze'],
        'Umwelt': ['Umwelt', 'Klima', 'Nachhaltigkeit', 'Energie']
    }
    
    theme_counts = {}
    all_text = ' '.join(df['analyse'].astype(str))
    
    for theme, keywords in themes.items():
        count = sum(all_text.lower().count(kw.lower()) for kw in keywords)
        if count > 0:
            theme_counts[theme] = count
    
    if theme_counts:
        theme_df = pd.DataFrame(list(theme_counts.items()), columns=['Thema', 'Erw√§hnungen'])
        theme_df = theme_df.sort_values('Erw√§hnungen', ascending=False)
        st.bar_chart(theme_df.set_index('Thema'))
    
    # Zeitverlauf
    st.markdown("### üìà Analysen im Zeitverlauf")
    df['datum_date'] = pd.to_datetime(df['datum']).dt.date
    timeline = df.groupby('datum_date').size().reset_index(name='Anzahl')
    if len(timeline) > 1:
        st.line_chart(timeline.set_index('datum_date'))

def automated_analysis_tab():
    """Tab f√ºr automatisierte Google Drive Analyse"""
    st.header("ü§ñ Automatisierte Zeitungsanalyse")
    
    # Eingebettete URL
    DEFAULT_WEB_APP_URL = "https://script.google.com/macros/s/AKfycbye1Pj2dOmadeRvFcZqLV-mIEGTcgwvxunVOncoRBBDQQUWfOngRfluEwYJew-cCiQ/exec"
    
    # Einfache Version - nur neueste PDF
    st.markdown("### üìÑ Neueste Zeitung automatisch analysieren")
    
    st.info("""
    ‚úÖ **Google Drive Integration aktiv!**
    
    Die App ist bereits mit deinem Zeitungsordner verbunden und kann:
    - Die neueste PDF automatisch erkennen
    - Sie herunterladen und analysieren
    - Duplikate vermeiden (bereits analysierte PDFs √ºberspringen)
    """)
    
    # URL ist voreingestellt aber √§nderbar
    web_app_url = st.text_input(
        "Web App URL (bereits konfiguriert):",
        value=st.session_state.get('web_app_url', DEFAULT_WEB_APP_URL),
        help="Die Standard-URL ist bereits eingestellt. Nur √§ndern wenn n√∂tig!"
    )
    
    if web_app_url:
        st.session_state.web_app_url = web_app_url
        
        # Hauptaktionen
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üîç Neueste PDF anzeigen", type="primary", use_container_width=True):
                check_newest_pdf(web_app_url)
        
        with col2:
            if st.button("üöÄ Jetzt analysieren", type="secondary", use_container_width=True):
                analyze_newest_pdf(web_app_url)
                
        with col3:
            if st.button("üîÑ Auto-Check (st√ºndlich)", type="secondary", use_container_width=True):
                st.session_state.auto_check = True
                auto_check_loop(web_app_url)
        
        # Erweiterte Optionen
        with st.expander("‚öôÔ∏è Erweiterte Optionen"):
            st.markdown("### üìÖ Zeitraum-Analyse")
            
            days_back = st.slider(
                "Analysiere PDFs der letzten X Tage:",
                min_value=1,
                max_value=30,
                value=7,
                help="Analysiert alle PDFs aus diesem Zeitraum"
            )
            
            if st.button("üìö Mehrere PDFs analysieren"):
                analyze_recent_pdfs(web_app_url, days_back)
            
            st.markdown("---")
            
            st.markdown("### üîß Script-Verwaltung")
            
            if st.button("üß™ Verbindung testen"):
                test_connection(web_app_url)
            
            st.markdown(f"""
            **Script-Editor √∂ffnen:**
            [Google Apps Script Editor ‚Üí](https://script.google.com)
            
            **Aktuelle Web App URL:**
            ```
            {web_app_url}
            ```
            """)

def check_newest_pdf(web_app_url):
    """Zeige Info √ºber die neueste PDF"""
    try:
        with st.spinner("Pr√ºfe neueste PDF..."):
            response = requests.get(web_app_url)
            data = response.json()
            
            if data.get('success'):
                file_info = data['file']
                modified = datetime.fromisoformat(file_info['modified'].replace('Z', '+00:00'))
                
                # Info-Box mit sch√∂nem Format
                info_col1, info_col2 = st.columns([2, 1])
                
                with info_col1:
                    st.success(f"""
                    ### üìÑ Neueste Zeitung gefunden:
                    **{file_info['name']}**
                    
                    üìÖ Hochgeladen: {modified.strftime('%d.%m.%Y um %H:%M Uhr')}
                    """)
                
                with info_col2:
                    # Pr√ºfe ob bereits analysiert
                    df = load_article_database()
                    if not df.empty and file_info['name'] in df['pdf_name'].values:
                        st.info("‚úÖ Bereits analysiert")
                    else:
                        st.warning("‚ö†Ô∏è Noch nicht analysiert")
                    
            else:
                st.error(f"Fehler: {data.get('error')}")
                
    except Exception as e:
        st.error(f"Verbindungsfehler: {e}")

def analyze_newest_pdf(web_app_url):
    """Analysiere nur die neueste PDF"""
    try:
        # API Key pr√ºfen
        api_key = st.secrets.get("GEMINI_API_KEY", "")
        if not api_key:
            st.error("‚ùå Gemini API Key fehlt! Bitte in Streamlit Secrets hinzuf√ºgen.")
            with st.expander("üîë So f√ºgst du den API Key hinzu:"):
                st.markdown("""
                1. Klicke auf **Manage app** (unten rechts)
                2. Gehe zu **Settings** ‚Üí **Secrets**
                3. F√ºge hinzu:
                ```toml
                GEMINI_API_KEY = "dein-api-key-hier"
                ```
                4. Speichern und App neu starten
                """)
            return
        
        # Progress Container
        progress_container = st.container()
        
        with progress_container:
            # Hole neueste PDF Info
            with st.spinner("üîç Suche neueste PDF..."):
                response = requests.get(web_app_url)
                data = response.json()
                
                if not data.get('success'):
                    st.error(f"Fehler: {data.get('error')}")
                    return
                
                file_info = data['file']
                
                # Pr√ºfe ob bereits analysiert
                df = load_article_database()
                if not df.empty and file_info['name'] in df['pdf_name'].values:
                    st.warning(f"‚ö†Ô∏è '{file_info['name']}' wurde bereits analysiert!")
                    if not st.checkbox("Trotzdem erneut analysieren?"):
                        return
            
            # Status-Updates
            status = st.empty()
            progress_bar = st.progress(0)
            
            # Download PDF
            status.text("üì• Lade PDF herunter...")
            progress_bar.progress(25)
            
            download_response = requests.post(
                web_app_url,
                data={'fileId': file_info['id']},
                timeout=60
            )
            
            if download_response.status_code != 200:
                st.error(f"Download-Fehler: HTTP {download_response.status_code}")
                return
            
            pdf_content = base64.b64decode(download_response.text)
            
            # PDF Buffer erstellen
            pdf_buffer = io.BytesIO(pdf_content)
            pdf_buffer.name = file_info['name']
            
            # Text extrahieren
            status.text("üìñ Extrahiere Text aus PDF...")
            progress_bar.progress(50)
            
            text = extract_pdf_text(pdf_buffer)
            
            if not text.strip():
                st.error("‚ùå Kein Text im PDF gefunden!")
                return
            
            # Analysieren
            status.text("ü§ñ KI analysiert relevante Artikel...")
            progress_bar.progress(75)
            
            analysis = analyze_with_gemini(text, api_key)
            
            # Speichern
            save_analysis_to_db(file_info['name'], analysis, text)
            
            progress_bar.progress(100)
            status.text("‚úÖ Analyse abgeschlossen!")
        
        # Ergebnis anzeigen
        st.success(f"‚úÖ **{file_info['name']}** erfolgreich analysiert!")
        st.markdown("---")
        
        # Analyse in sch√∂nem Format
        with st.container():
            st.markdown(analysis)
        
        # Download-Buttons
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                label="üì• Als Markdown speichern",
                data=f"# JL Zeitungsanalyse\n\n**Datei:** {file_info['name']}\n**Datum:** {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n{analysis}",
                file_name=f"JL_Analyse_{file_info['name'].replace('.pdf', '')}_{datetime.now().strftime('%Y%m%d')}.md",
                mime="text/markdown",
                use_container_width=True
            )
        
        with col2:
            # Als PDF speichern (sp√§ter implementieren)
            st.button("üìÑ Als PDF speichern", disabled=True, use_container_width=True, help="Kommt bald!")
        
    except Exception as e:
        st.error(f"Fehler: {e}")
        with st.expander("üêõ Fehlerdetails"):
            import traceback
            st.code(traceback.format_exc())

def analyze_recent_pdfs(web_app_url, days):
    """Analysiere alle PDFs der letzten X Tage"""
    st.info(f"üìÖ Suche PDFs der letzten {days} Tage...")
    
    # Hier w√ºrde die Logik f√ºr mehrere PDFs kommen
    # F√ºr jetzt zeigen wir nur eine Info
    st.warning("Diese Funktion ist noch in Entwicklung!")
    
def test_connection(web_app_url):
    """Teste die Verbindung zum Google Apps Script"""
    try:
        with st.spinner("Teste Verbindung..."):
            response = requests.get(web_app_url)
            
            col1, col2 = st.columns(2)
            
            with col1:
                if response.status_code == 200:
                    st.success("‚úÖ Verbindung erfolgreich!")
                else:
                    st.error(f"‚ùå HTTP Fehler: {response.status_code}")
            
            with col2:
                try:
                    data = response.json()
                    if data.get('success'):
                        st.success("‚úÖ Script funktioniert!")
                    else:
                        st.warning("‚ö†Ô∏è Script-Fehler")
                except:
                    st.error("‚ùå Keine g√ºltige Antwort")
            
            with st.expander("üîç Technische Details"):
                st.json(response.json() if response.status_code == 200 else {"error": response.text})
                
    except Exception as e:
        st.error(f"Verbindungsfehler: {e}")

def auto_check_loop(web_app_url):
    """Automatische st√ºndliche √úberpr√ºfung"""
    st.info("üîÑ Automatische √úberpr√ºfung aktiviert - pr√ºft st√ºndlich auf neue PDFs")
    
    # Placeholder f√ºr zuk√ºnftige Implementation
    st.warning("Diese Funktion l√§uft nur solange die App ge√∂ffnet ist. F√ºr echte Automatisierung nutze einen Cron-Job oder GitHub Actions.")

def check_newest_pdf(web_app_url):
    """Zeige Info √ºber die neueste PDF"""
    try:
        with st.spinner("Pr√ºfe neueste PDF..."):
            response = requests.get(web_app_url)
            data = response.json()
            
            if data.get('success'):
                file_info = data['file']
                modified = datetime.fromisoformat(file_info['modified'].replace('Z', '+00:00'))
                
                st.success(f"""
                ### üìÑ Neueste PDF gefunden:
                - **Name:** {file_info['name']}
                - **Ge√§ndert:** {modified.strftime('%d.%m.%Y %H:%M')}
                - **ID:** {file_info['id']}
                """)
                
                # Pr√ºfe ob bereits analysiert
                df = load_article_database()
                if not df.empty and file_info['name'] in df['pdf_name'].values:
                    st.info("‚ÑπÔ∏è Diese PDF wurde bereits analysiert!")
                else:
                    st.warning("‚ö†Ô∏è Diese PDF wurde noch nicht analysiert!")
                    
            else:
                st.error(f"Fehler: {data.get('error')}")
                
    except Exception as e:
        st.error(f"Fehler: {e}")

def analyze_newest_pdf(web_app_url):
    """Analysiere nur die neueste PDF"""
    try:
        # API Key pr√ºfen
        api_key = st.secrets.get("GEMINI_API_KEY", "")
        if not api_key:
            st.error("‚ùå Gemini API Key fehlt!")
            return
        
        # Hole neueste PDF Info
        with st.spinner("Hole neueste PDF..."):
            response = requests.get(web_app_url)
            data = response.json()
            
            if not data.get('success'):
                st.error(f"Fehler: {data.get('error')}")
                return
            
            file_info = data['file']
            st.info(f"üìÑ Verarbeite: {file_info['name']}")
        
        # Download PDF
        with st.spinner("Lade PDF herunter..."):
            download_response = requests.post(
                web_app_url,
                data={'fileId': file_info['id']},
                timeout=60
            )
            
            if download_response.status_code != 200:
                st.error(f"Download-Fehler: HTTP {download_response.status_code}")
                return
            
            # Pr√ºfe ob Fehler-JSON
            try:
                error_check = download_response.json()
                if 'error' in error_check:
                    st.error(f"Script-Fehler: {error_check['error']}")
                    return
            except:
                # Kein JSON = gut (Base64 Content)
                pass
            
            pdf_content = base64.b64decode(download_response.text)
            st.success(f"‚úÖ Download erfolgreich ({len(pdf_content):,} Bytes)")
        
        # PDF Buffer erstellen
        pdf_buffer = io.BytesIO(pdf_content)
        pdf_buffer.name = file_info['name']
        
        # Text extrahieren
        with st.spinner("Extrahiere Text..."):
            text = extract_pdf_text(pdf_buffer)
            
            if not text.strip():
                st.error("‚ùå Kein Text im PDF gefunden!")
                return
        
        # Analysieren
        with st.spinner("ü§ñ KI analysiert Artikel..."):
            analysis = analyze_with_gemini(text, api_key)
        
        # Speichern
        if save_analysis_to_db(file_info['name'], analysis, text):
            st.success("üíæ In Database gespeichert!")
        
        # Ergebnis anzeigen
        st.success("‚úÖ Analyse abgeschlossen!")
        st.markdown("---")
        st.markdown(analysis)
        
        # Download
        st.download_button(
            label="üì• Analyse herunterladen",
            data=f"# JL Zeitungsanalyse\n\n**Datei:** {file_info['name']}\n**Datum:** {datetime.now().strftime('%d.%m.%Y %H:%M')}\n\n{analysis}",
            file_name=f"JL_Analyse_{file_info['name'].replace('.pdf', '')}_{datetime.now().strftime('%Y%m%d')}.md",
            mime="text/markdown"
        )
        
    except Exception as e:
        st.error(f"Fehler: {e}")
        import traceback
        st.code(traceback.format_exc())

def apps_script_integration():
    """Integration √ºber Google Apps Script f√ºr private Ordner"""
    st.markdown("### üîê Private Ordner-Integration mit Google Apps Script")
    
    with st.expander("üìñ Einrichtungsanleitung", expanded=True):
        st.markdown("""
        **So richtest du Google Apps Script ein:**
        
        1. **√ñffne Google Drive** und gehe zu deinem Zeitungsordner
        2. **Erstelle ein neues Apps Script**: Rechtsklick ‚Üí Mehr ‚Üí Google Apps Script
        3. **L√∂sche allen vorhandenen Code und kopiere diesen Code:**
        
        ```javascript
        function doGet(e) {
          try {
            // WICHTIG: Ersetze diese ID mit deiner Ordner-ID!
            const folderId = 'DEINE_ORDNER_ID_HIER';
            
            const folder = DriveApp.getFolderById(folderId);
            const files = folder.getFilesByType(MimeType.PDF);
            
            const fileList = [];
            const cutoffDate = new Date();
            cutoffDate.setDate(cutoffDate.getDate() - 7); // Letzte 7 Tage
            
            while (files.hasNext()) {
              const file = files.next();
              if (file.getLastUpdated() > cutoffDate) {
                fileList.push({
                  id: file.getId(),
                  name: file.getName(),
                  modified: file.getLastUpdated().toISOString()
                });
              }
            }
            
            // Wichtig: Als JSON zur√ºckgeben
            return ContentService
              .createTextOutput(JSON.stringify(fileList))
              .setMimeType(ContentService.MimeType.JSON);
              
          } catch (error) {
            // Fehler als JSON zur√ºckgeben
            return ContentService
              .createTextOutput(JSON.stringify({
                error: error.toString(),
                message: "Fehler beim Abrufen der Dateien"
              }))
              .setMimeType(ContentService.MimeType.JSON);
          }
        }
        
        function doPost(e) {
          try {
            const fileId = e.parameter.fileId;
            const file = DriveApp.getFileById(fileId);
            const content = file.getBlob().getBytes();
            
            return ContentService
              .createTextOutput(Utilities.base64Encode(content))
              .setMimeType(ContentService.MimeType.TEXT);
              
          } catch (error) {
            return ContentService
              .createTextOutput(JSON.stringify({
                error: error.toString()
              }))
              .setMimeType(ContentService.MimeType.JSON);
          }
        }
        ```
        
        4. **Finde deine Ordner-ID:**
           - √ñffne deinen Zeitungsordner
           - URL: `https://drive.google.com/drive/folders/XXXXX`
           - Kopiere `XXXXX` (das ist deine Ordner-ID)
           - Ersetze `DEINE_ORDNER_ID_HIER` im Script
        
        5. **Speichern und Deploy:**
           - Strg+S zum Speichern
           - Deploy ‚Üí New Deployment
           - Type: **Web app**
           - Execute as: **Me**
           - Who has access: **Anyone**
           - Deploy klicken
           
        6. **WICHTIG: Autorisierung**
           - Beim ersten Deploy: "Review Permissions" klicken
           - W√§hle dein Google-Konto
           - "Advanced" ‚Üí "Go to [Scriptname] (unsafe)"
           - "Allow" klicken
           
        7. **Kopiere die Web App URL** (beginnt mit https://script.google.com/macros/s/...)
        
        **Teste die URL:**
        - √ñffne die URL in einem neuen Browser-Tab
        - Du solltest JSON-Daten sehen (Array mit Dateien)
        - Wenn Fehler: √úberpr√ºfe die Ordner-ID
        """)
    
    # Test-Button f√ºr direkte URL
    st.markdown("### üß™ Script testen")
    
    test_url = st.text_input(
        "Teste deine Web App URL direkt:",
        placeholder="https://script.google.com/macros/s/.../exec"
    )
    
    if test_url and st.button("üîç URL testen"):
        try:
            with st.spinner("Teste URL..."):
                response = requests.get(test_url)
                
                st.info(f"HTTP Status: {response.status_code}")
                
                # Zeige Raw Response
                with st.expander("Raw Response anzeigen"):
                    st.code(response.text[:1000])
                
                # Versuche als JSON zu parsen
                try:
                    data = response.json()
                    st.success("‚úÖ G√ºltiges JSON erkannt!")
                    
                    if isinstance(data, list):
                        st.write(f"Gefundene Dateien: {len(data)}")
                        if data:
                            st.write("Erste Datei:", data[0])
                    elif 'error' in data:
                        st.error(f"Script-Fehler: {data['error']}")
                    else:
                        st.json(data)
                        
                except:
                    st.error("‚ùå Kein g√ºltiges JSON. √úberpr√ºfe das Script!")
                    
        except Exception as e:
            st.error(f"Verbindungsfehler: {e}")
    
    # Web App URL eingeben
    web_app_url = st.text_input(
        "Google Apps Script Web App URL:",
        placeholder="https://script.google.com/macros/s/.../exec",
        help="Die URL aus dem Deployment"
    )
    
    if web_app_url:
        if st.button("üìÇ Dateien abrufen", type="primary"):
            fetch_and_analyze_apps_script(web_app_url)

def fetch_and_analyze_apps_script(web_app_url):
    """Hole und analysiere Dateien √ºber Apps Script"""
    try:
        # API Key pr√ºfen
        api_key = st.secrets.get("GEMINI_API_KEY", "")
        if not api_key:
            st.error("‚ùå Gemini API Key fehlt in den Secrets!")
            return
        
        # Dateien abrufen
        with st.spinner("üìÇ Hole Dateiliste..."):
            response = requests.get(web_app_url)
            
            # Debug: Zeige Raw Response
            if response.status_code != 200:
                st.error(f"‚ùå HTTP Fehler: {response.status_code}")
                st.code(response.text[:500])
                return
            
            # Versuche JSON zu parsen
            try:
                files = response.json()
            except:
                st.error("‚ùå Antwort ist kein g√ºltiges JSON")
                st.code(response.text[:500])
                
                # Versuche HTML-Fehler zu erkennen
                if "<!DOCTYPE" in response.text or "<html" in response.text:
                    st.error("""
                    ‚ö†Ô∏è Das Script gibt HTML zur√ºck statt JSON.
                    
                    **M√∂gliche Ursachen:**
                    1. Die Web App URL ist falsch
                    2. Das Script wurde nicht deployed
                    3. Zugriffsfehler
                    
                    **L√∂sung:** 
                    - √úberpr√ºfe die URL
                    - Stelle sicher, dass das Script deployed ist
                    - Teste die URL direkt im Browser
                    """)
                return
        
        if not files:
            st.warning("Keine Dateien gefunden")
            return
            
        st.success(f"‚úÖ {len(files)} PDF(s) gefunden")
        
        # Zeige Dateien
        with st.expander("üìã Gefundene Dateien"):
            for file in files:
                st.write(f"üìÑ {file.get('name', 'Unbekannt')} ({file.get('size', 0):,} Bytes)")
        
        # Container f√ºr Live-Updates
        status_container = st.container()
        
        # Analyse-Button direkt sichtbar
        if st.button("üöÄ Alle Dateien analysieren", type="primary", key="analyze_all"):
            
            # Progress-Elemente
            progress_bar = st.progress(0)
            status_text = st.empty()
            log_container = st.container()
            
            successful = 0
            failed = 0
            all_analyses = []
            
            st.write("üìä Starte Analyse...")
            
            for idx, file in enumerate(files):
                progress = (idx + 1) / len(files)
                progress_bar.progress(progress)
                
                file_name = file.get('name', 'Unbekannt')
                status_text.text(f"Verarbeite {idx + 1}/{len(files)}: {file_name}")
                
                with log_container:
                    with st.expander(f"üìÑ {file_name}", expanded=True):
                        try:
                            # Schritt 1: Download
                            st.write("‚è≥ Lade PDF herunter...")
                            
                            download_response = requests.post(
                                web_app_url,
                                data={'fileId': file['id']},
                                timeout=30
                            )
                            
                            if download_response.status_code != 200:
                                st.error(f"‚ùå Download-Fehler: HTTP {download_response.status_code}")
                                failed += 1
                                continue
                            
                            # Check if response is JSON (error)
                            if download_response.headers.get('content-type', '').startswith('application/json'):
                                error_data = download_response.json()
                                st.error(f"‚ùå Script-Fehler: {error_data.get('error', 'Unbekannter Fehler')}")
                                failed += 1
                                continue
                            
                            st.write(f"‚úÖ Download erfolgreich ({len(download_response.content):,} Bytes)")
                            
                            # Schritt 2: Base64 Decode
                            try:
                                pdf_content = base64.b64decode(download_response.text)
                                st.write(f"‚úÖ Dekodierung erfolgreich ({len(pdf_content):,} Bytes)")
                            except Exception as e:
                                st.error(f"‚ùå Base64-Dekodierung fehlgeschlagen: {e}")
                                failed += 1
                                continue
                            
                            # Schritt 3: PDF Buffer erstellen
                            pdf_buffer = io.BytesIO(pdf_content)
                            pdf_buffer.name = file_name
                            
                            # Schritt 4: Text extrahieren
                            st.write("‚è≥ Extrahiere Text aus PDF...")
                            text = extract_pdf_text(pdf_buffer)
                            
                            if not text.strip():
                                st.warning("‚ö†Ô∏è Kein Text im PDF gefunden")
                                failed += 1
                                continue
                            
                            st.write(f"‚úÖ Text extrahiert ({len(text):,} Zeichen)")
                            
                            # Schritt 5: Analysieren
                            st.write("ü§ñ Analysiere mit KI...")
                            analysis = analyze_with_gemini(text, api_key)
                            st.write("‚úÖ Analyse abgeschlossen")
                            
                            # Schritt 6: Speichern
                            if save_analysis_to_db(file_name, analysis, text):
                                st.write("üíæ In Datenbank gespeichert")
                            
                            all_analyses.append({
                                'filename': file_name,
                                'date': datetime.now().strftime('%d.%m.%Y %H:%M'),
                                'analysis': analysis
                            })
                            
                            successful += 1
                            st.success(f"‚úÖ Erfolgreich analysiert!")
                            
                            # Kurze Pause f√ºr API Rate Limits
                            time.sleep(1)
                            
                        except requests.Timeout:
                            st.error("‚ùå Zeit√ºberschreitung beim Download")
                            failed += 1
                        except Exception as e:
                            st.error(f"‚ùå Unerwarteter Fehler: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                            failed += 1
            
            # Finale Zusammenfassung
            progress_bar.progress(1.0)
            status_text.text("‚úÖ Batch-Analyse abgeschlossen!")
            
            st.markdown("---")
            st.success(f"""
            ### üìä Zusammenfassung:
            - ‚úÖ **Erfolgreich:** {successful} PDFs
            - ‚ùå **Fehlgeschlagen:** {failed} PDFs
            - üìÅ **Gesamt:** {len(files)} PDFs
            """)
            
            # Bericht erstellen wenn Analysen vorhanden
            if all_analyses:
                report = create_batch_report(all_analyses)
                
                st.markdown("---")
                with st.expander("üìÑ Gesamtbericht anzeigen", expanded=True):
                    st.markdown(report)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="üì• Bericht als Markdown",
                        data=report,
                        file_name=f"JL_Batch_{datetime.now().strftime('%Y%m%d_%H%M')}.md",
                        mime="text/markdown"
                    )
                
                with col2:
                    # CSV Export
                    df = pd.DataFrame(all_analyses)
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="üì• Ergebnisse als CSV",
                        data=csv,
                        file_name=f"JL_Analysen_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                        mime="text/csv"
                    )
            else:
                st.warning("Keine erfolgreichen Analysen zum Exportieren")
                
    except Exception as e:
        st.error(f"Kritischer Fehler: {e}")
        import traceback
        st.code(traceback.format_exc())

def public_folder_integration():
    """Integration f√ºr √∂ffentliche Google Drive Ordner"""
    st.markdown("### üìÇ √ñffentlicher Ordner (Vereinfachte Methode)")
    
    st.info("""
    ‚ö†Ô∏è Diese Methode funktioniert nur mit √∂ffentlich freigegebenen Ordnern!
    
    F√ºr private Ordner nutze bitte die Google Apps Script Methode oben.
    """)
    
    gdrive_url = st.text_input(
        "√ñffentlicher Google Drive Ordner URL:",
        placeholder="https://drive.google.com/drive/folders/..."
    )
    
    if gdrive_url:
        st.warning("Diese Funktion ist noch nicht implementiert f√ºr private Ordner.")
        st.info("Bitte nutze die Google Apps Script Methode oben!")

def manual_batch_upload():
    """Manueller Batch-Upload von mehreren PDFs"""
    st.markdown("### üì§ Manueller Batch-Upload")
    
    # API Key pr√ºfen
    api_key = st.secrets.get("GEMINI_API_KEY", "")
    if not api_key:
        st.error("‚ùå Gemini API Key fehlt in den Secrets!")
        return
    
    # Mehrere PDFs hochladen
    pdf_files = st.file_uploader(
        "W√§hle mehrere PDFs aus:",
        type=['pdf'],
        accept_multiple_files=True,
        help="Du kannst mehrere Dateien gleichzeitig ausw√§hlen"
    )
    
    if pdf_files:
        st.success(f"‚úÖ {len(pdf_files)} PDFs ausgew√§hlt")
        
        # Liste anzeigen
        with st.expander("üìã Ausgew√§hlte Dateien"):
            for pdf in pdf_files:
                st.write(f"üìÑ {pdf.name} ({pdf.size:,} Bytes)")
        
        # Analyse starten
        if st.button("üöÄ Batch-Analyse starten", type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            successful = 0
            all_analyses = []
            
            for idx, pdf_file in enumerate(pdf_files):
                progress = (idx + 1) / len(pdf_files)
                progress_bar.progress(progress)
                status_text.text(f"Analysiere: {pdf_file.name}")
                
                try:
                    # PDF lesen
                    pdf_file.seek(0)
                    text = extract_pdf_text(pdf_file)
                    
                    if text.strip():
                        # Analysieren
                        analysis = analyze_with_gemini(text, api_key)
                        
                        # Speichern
                        save_analysis_to_db(pdf_file.name, analysis, text)
                        
                        all_analyses.append({
                            'filename': pdf_file.name,
                            'date': datetime.now().strftime('%d.%m.%Y %H:%M'),
                            'analysis': analysis
                        })
                        
                        successful += 1
                        
                except Exception as e:
                    st.error(f"Fehler bei {pdf_file.name}: {e}")
            
            # Ergebnis
            progress_bar.progress(1.0)
            status_text.text("‚úÖ Batch-Analyse abgeschlossen!")
            
            st.success(f"""
            ### üìä Ergebnis:
            - ‚úÖ Erfolgreich: {successful} PDFs
            - ‚ùå Fehlgeschlagen: {len(pdf_files) - successful} PDFs
            """)
            
            # Bericht
            if all_analyses:
                report = create_batch_report(all_analyses)
                
                with st.expander("üìÑ Gesamtbericht", expanded=True):
                    st.markdown(report)
                
                st.download_button(
                    label="üì• Bericht herunterladen",
                    data=report,
                    file_name=f"JL_Batch_{datetime.now().strftime('%Y%m%d_%H%M')}.md",
                    mime="text/markdown"
                )

def create_batch_report(analyses):
    """Erstelle Batch-Analyse Bericht"""
    report = f"""# ü§ñ JL BATCH-ANALYSE BERICHT

**Datum:** {datetime.now().strftime('%d.%m.%Y %H:%M')}
**Anzahl Zeitungen:** {len(analyses)}

---

## üìä ZUSAMMENFASSUNG

"""
    
    # Z√§hle Priorit√§ten
    total_highest = 0
    total_high = 0
    
    for analysis_data in analyses:
        total_highest += analysis_data['analysis'].count('üî•')
        total_high += analysis_data['analysis'].count('‚ö°')
    
    report += f"""
- üî• **H√∂chste Priorit√§t gesamt:** {total_highest} Artikel
- ‚ö° **Hohe Priorit√§t gesamt:** {total_high} Artikel
- üì∞ **Analysierte Zeitungen:** {len(analyses)}

---

## üì∞ EINZELANALYSEN
"""
    
    for analysis_data in analyses:
        report += f"\n### üìÑ {analysis_data['filename']}\n"
        report += f"*Analysiert: {analysis_data['date']}*\n\n"
        report += analysis_data['analysis']
        report += "\n\n---\n"
    
    return report

def main_app():
    """Hauptanwendung nach Login"""
    st.title("üì∞ JL Zeitungsanalyse f√ºr Kommunalpolitik")
    st.markdown("*Finde relevante Artikel f√ºr liberale Politik auf einen Blick*")
    
    # User Info & Logout
    col1, col2 = st.columns([6, 1])
    with col2:
        st.markdown(f"üë§ {st.session_state.get('username', 'User')}")
        if st.button("üö™ Logout", use_container_width=True):
            st.session_state.logged_in = False
            st.rerun()
    
    # Tab-Navigation
    tab1, tab2, tab3, tab4 = st.tabs(["üì§ Neue Analyse", "üîç Artikel-Suche", "üìä Statistiken", "ü§ñ Automatisierung"])
    
    with tab1:
        analyze_tab()
    
    with tab2:
        search_tab()
        
    with tab3:
        stats_tab()
    
    with tab4:
        # Importiere die Automatisierungs-Funktionen
        automated_analysis_tab()

def main():
    """Hauptfunktion mit Session State Management"""
    
    # Page Config MUSS als allererstes kommen
    st.set_page_config(
        page_title="JL Zeitungsanalyse",
        page_icon="üì∞",
        layout="wide"
    )
    
    # Session State initialisieren
    if 'logged_in' not in st.session_state:
        st.session_state.logged_in = False
    
    # App-Routing
    if st.session_state.logged_in:
        main_app()
    else:
        show_login()

# App starten
if __name__ == "__main__":
    main()
